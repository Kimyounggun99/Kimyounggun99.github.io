<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  <!-- Favicon code from realfavicongenerator.net -->
  <link rel="apple-touch-icon" type="image/png" sizes="16x16" href="/assets/img/icons/rocket_icon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/rocket_icon.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
  <meta name="msapplication-TileColor" content="#563d7c">
  <meta name="theme-color" content="#ffffff">

  <!--jQuery-->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"
    integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"
    integrity="sha384-oBqDVmMz9ATKxIep9tiCxS/Z9fNfEXiDAYTujMAeBAsjFuCZSmKbSSUnQlmh/jp3"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.min.js"
    integrity="sha384-7VPbUDkoPSGFnVtYi0QogXtr74QeVeeIs99Qfg5YCF+TidwNdjvaKZX19NZ/e6oz"
    crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css"
    integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css"
    integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
  <title>Projects | Younggun Kim</title>
  <meta name="generator" content="Jekyll v4.2.2" />
  <meta property="og:title" content="projects" />
  <meta name="author" content="Younggun Kim" />
  <meta property="og:locale" content="en_GB" />
  <meta name="description" content="List of held projects" />
  <meta property="og:description" content="List of held projects" />
  <link rel="canonical" href="http://localhost:4000/projects.html" />
  <meta property="og:url" content="http://localhost:4000/projects.html" />
  <meta property="og:site_name" content="Younggun Kim" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta property="twitter:title" content="projects" />
  <meta name="twitter:site" content="@liu_puze" />
  <meta name="twitter:creator" content="@Younggun Kim" />
  <script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Younggun Kim"},"description":"List of held projects","headline":"projects","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/head.png"},"name":"Younggun Kim"},"url":"http://localhost:4000/projects.html"}</script>
  <!-- End Jekyll SEO tag -->

</head>

<body>
  <div class="container">


    <section id="header-nav">
      <header>
        <nav class="navbar navbar-expand-md bg-light" style="background-color: #ffffff;">
          <div class="container-fluid mt-3">
            <!-- <image class="icon" src="/assets/img/icons/rocket_icon.png"></image> -->
            <button class="navbar-toggler my-auto" type="button" data-bs-toggle="collapse"
              data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
              aria-label="Toggle navigation">
              <span><i class="fas fa-bars"></i></span>
            </button>

            <div class="collapse navbar-collapse mb-0 pb-3" id="navbarSupportedContent">
              <ul class="navbar-nav">

                <li class="nav-item btn-nav my-auto mx-auto pe-3 text-center ">
                  <a class="nav-link" href="/">
                    <span data-hover="Home">Home</span>
                  </a>
                </li>

                <li class="nav-item btn-nav my-auto mx-auto pe-3 text-center ">
                  <a class="nav-link" href="/publications.html">
                    <span data-hover="Publications">Publications</span>
                  </a>
                </li>

                <li class="nav-item my-auto mx-auto pe-3 text-center current"><a class="nav-link active"
                    aria-current="page" href="">Projects</a></li>

                <!-- blog -->

              </ul>
            </div>
          </div>
        </nav>
      </header>
    </section>
  </div>

  <section class="page-title-bar" style="background-color: #2f6d7e; padding: 40px 0;">
    <div class="container text-center">
      <h1 style="color: white; font-weight: 600; font-size: 2.2rem; margin: 0;">Projects</h1>
    </div>
  </section>


<section id="recentpubs" class="recentpubs" style="background-color: #F4F3EA;">
  <div class="recentpubs-wrap">
    <p style="font-weight: bold; color: #000; font-size: 1.6rem;">Intelligent Transportation System Development</p>
    <h3 class="bibliography">[1] Pedestrian Crossing Direction Prediction at Intersections for Pedestrian Safety</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim*</em></strong></span>, Mohamed Abdel-Aty, Keechoo Choi, Zubayer Islam, Dongdong Wang, and Shaoyan Zhai</p>
    <p><em>IEEE Open Journal of Intelligent Transportation Systems (OJ-ITS), 2025. [Impact Factor: 5.3]</em></p>
    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/OJ_ITS_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> With the rise of smart intersections, accurately predicting a pedestrianâ€™s crossing direction at the intersection level is essential for enhancing pedestrian safety and optimizing traffic signal control. However, this task is challenging due to the diverse configurations of real-world intersections, each with different crosswalk orientations, geographic layouts, and CCTV placements and angles.</p>

      </div>

       <!-- Row 2: Methodology (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> To address this, we propose a geometric-invariant space embedding method. As illustrated in the video, our approach enables pedestrians captured at different intersections, with varying CCTV angles and crosswalk orientations, to be embedded into a unified spatial representation. This standardization allows robust learning of crossing behaviors. Furthermore, we adopt a Transformer-based encoder, achieving accuracy of 94.10% and an F1-score of 92.35%.</p>
        </div>
      </div>
    
      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> The proposed method demonstrates strong potential for real-world deployment. Its robustness to intersection variability makes it suitable for integration into city-wide intelligent traffic management systems to proactively ensure pedestrian safety at signalized intersections.</p>
        
          <div class="links mt-2">
            [<a href="https://ieeexplore.ieee.org/document/11016106" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
        </div>
      </div>
    </div>



    <h3 class="bibliography">[2] CCTV-Drone Calibration for Surrogate Safety Measure of Vehicles at Smart Intersections</h3>
    <p><strong><em>On going project</em></strong>
    <div class="row align-items-start">
      
      <!-- Drone_CCTV_Video.mp4 -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/Drone_CCTV_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> In smart intersections, accurately assessing vehicle-to-vehicle safety is crucial. This requires estimating <em>safety surrogate measures</em> such as relative distance. However, limited CCTV viewpoints, typically front-facing, lack depth information, making it infeasible to compute inter-vehicle distances directly from monocular footage.</p>
      </div>

       <!-- Row 2: Methodology (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> We calibrate top-down drone views with CCTV footage to construct vehicle bounding boxes in both front and bottom perspectives. By calibrating vehicles captured in drone views with those in CCTV footage, we develop a cross-view dataset that includes bottom bounding boxes. A model is then trained to estimate the both bounding boxes from CCTV-only views, enabling accurate spatial localization without relying on drone footage. </p>
        </div>
      </div>
    
      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong>  This work enables real-time vehicle safety monitoring using only existing CCTV infrastructure. By bridging the gap between aerial and CCTV footage, it allows accurate estimation of surrogate safety measures at intersections, contributing to proactive vehicle conflict detection and urban traffic safety management.</p>
        
        
        </div>
      </div>
    </div>




    <h3 class="bibliography">[3] Joint Trajectory Prediction of Vehicles and Pedestrians for Safety at Intersections</h3>
    <p><strong><em> Project under the National Science Foundation(NSF) and Center for Smart Streetscapes (CS3)</em></strong>
    <div class="row align-items-start">
      
      <!-- Drone_CCTV_Video.mp4 -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/Trajectory_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> Vehicles and pedestrians continuously influence each other's movement, particularly through behaviors such as yielding or hesitation. Accurately predicting the future trajectories of both agents is essential for assessing <em>surrogate safety measures (SSM)</em> at intersections. A joint understanding enables proactive conflict detection and enhances traffic safety. </p>

         <p><strong>Methodology:</strong> We leverage a Graph Convolutional Network (GCN)-based framework to jointly encode the historical trajectories of vehicles and pedestrians into a shared latent feature space. The model captures mutual interactions between the two agent types and learns to predict their future trajectories concurrently.</p>
      </div>

      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> Our approach facilitates accurate multi-agent trajectory forecasting at intersections using observed movement patterns. By modeling pedestrianâ€“vehicle interactions, it enhances the reliability of safety assessment and supports the development of smarter traffic control systems that account for both pedestrians and vehicles.</p>
        
        
        </div>
      </div>
    </div>



  </div>
</section>






<section id="recentpubs" class="recentpubs" >
  <div class="recentpubs-wrap">
    <p style="font-weight: bold; color: #000; font-size: 1.6rem;">LiDAR-Based Object Recognition for Autonomous Vehicles </p>
    <h3 class="bibliography">[1] 3D Adaptive Structural Convolution Network for Domain-Invariant Point Cloud Recognition</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span> and Soomok Lee*</p>
    <p><em>Asian Conference on Computer Vision, 2024. [BK21(Brain Korea) Distinguished Conference Paper List] </em></p>
    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <img src="/assets/img/ASCN.png" alt="ASCN Figure" style="width: 100%; height: auto;" />
      </div>

      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> LiDAR is one of the crucial sensors for autonomous vehicles (AVs), offering accurate 3D spatial information essential for object recognition and safe navigation. However, the quality of LiDAR point clouds varies significantly with the number of channels (e.g., 32CH, 64CH, 128CH), and high-channel sensors are often too expensive for wide deployment. This creates a challenge, as deep learning models trained on high-resolution data often underperform when applied to lower-resolution inputs. Thus, it is critical to develop recognition models that are robust to LiDAR channel variations. </p>

      </div>

       <!-- Row 2: Methodology (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> We developed an AI model, named 3D-ASCN, that recognizes objects using 3D point cloud data from LiDAR sensors. To address the challenges caused by variations in LiDAR sensor configurations, our model is designed to maintain reliable performance even when the quality or resolution of LiDAR data changes. Specifically, we introduce a distance-based kernel and a direction-based kernel that learn the structural feature representations of objects from point clouds. These kernels enable the model to focus on the intrinsic geometry of objects rather than the density of the point cloud itself, allowing for robust classification and object detection across different LiDAR resolutions.</p>
        </div>
      </div>
    
      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> The proposed 3D-ASCN can contribute to the real-world deployment of AVs by enabling reliable object recognition regardless of the LiDAR sensor used. By overcoming performance degradation caused by LiDAR channel shifts, our approach reduces hardware dependency and cost, allowing AV systems to be both scalable and economically feasible. </p>
        
          <div class="links mt-2">
            [<a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Kim_3D_Adaptive_Structural_Convolution_Network_for_Domain-Invariant_Point_Cloud_Recognition_ACCV_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
        </div>
      </div>
    </div>
    


    <h3 class="bibliography">[2] Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span>, Beomsik Cho, Seonghoon Ryoo, and Soomok Lee*</p>
    <p><em>This work has been submitted to Expert Systems with Applications.(ESWA) [Impact Factor: 7.5] </em></p>
    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <img src="/assets/img/MSCN.png" alt="MSCN Figure" style="width: 100%; height: auto;" />
      </div>

      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
        <ul style="padding-left: 1.2em; margin-top: 0.4rem;">
          <li style="margin-bottom: 0.4rem;">
            We developed MSCN, an improved version of the previously proposed 3D-ASCN model, which robustly recognizes objects from point clouds under both LiDAR channel variations and domain shifts between simulation and real-world environments.
          </li>
          <li style="margin-bottom: 0.4rem;">
            While 3D-ASCN focuses on local structural features centered around each point, MSCN extends this by capturing both <span style="text-decoration: underline;">local and global structural features</span>, leading to improved recognition performance and robustness to domain shifts.
          </li>
          <li style="margin-bottom: 0.4rem;">
            To evaluate the modelâ€™s recognition ability under simulation-to-real-world (Sim-to-Real) domain shifts, we developed a synthetic point cloud dataset. This dataset enabled rigorous testing of sim-to-real generalization, validating MSCNâ€™s robustness in diverse deployment scenarios.
          </li>
          <li style="margin-bottom: 0.4rem;">
            <strong>Impact:</strong> MSCN enables cost-effective deployment of AV systems by maintaining strong recognition performance across both varying LiDAR sensor configurations (e.g., low-cost vs. high-resolution sensors) and domain shifts between synthetic and real-world data, addressing key challenges in both sensor variability and data availability.
          </li>
        </ul>

      </div>

      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
          <div class="links mt-2">
            [<a href="https://arxiv.org/abs/2501.16289" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
      </div>
    </div>



  </div>
</section>






  

  <!-- Footer -->
  <footer>
    <div class="footer" style="font-size: 1.2rem;">
      Contact: younggun.kim@ucf.edu
    </div>
  </footer>


  <script type="text/javascript" src="/assets/js/jquery-3.3.1.min.js"></script>
  <script type="text/javascript" src="/assets/js/Headroom.js"></script>
  <script type="text/javascript" src="/assets/js/jQuery.headroom.js"></script>
  <script type="text/javascript" src="/assets/js/custom.js"></script>


  <script>
    var divs = document.querySelectorAll("div.project-content");
    $('div.project-content').each(function (index) {
      var filename = $(this).text();
      filename = filename.replace(/\s/g, '');
      filename = filename.replace("/_posts/", "");
      filename = filename.replace(".md", ".html");
      filename = filename.substring(0, 4) + "/" + filename.substring(5);
      filename = filename.substring(0, 7) + "/" + filename.substring(8);
      filename = filename.substring(0, 10) + "/" + filename.substring(11);

      console.log(filename);
      $.ajax({
        url: filename, dataType: "html", success: function (data) {
          const node = new DOMParser().parseFromString(data, "text/html");
          divs[index].innerHTML = node.getElementById("post").getElementsByClassName("post-content")[0].innerHTML;
        }
      })
    });

  </script>
  </div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz"
    crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        // customised options
        // â€¢ auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true },
          { left: '$', right: '$', display: false },
          { left: '\\(', right: '\\)', display: false },
          { left: '\\[', right: '\\]', display: true }
        ],
        // â€¢ rendering keys, e.g.:
        throwOnError: false
      });
    });
  </script>
  <script type="module">
    // Import the functions you need from the SDKs you need
    import { initializeApp } from "https://www.gstatic.com/firebasejs/9.10.0/firebase-app.js";
    import { getAnalytics } from "https://www.gstatic.com/firebasejs/9.10.0/firebase-analytics.js";
    // TODO: Add SDKs for Firebase products that you want to use
    // https://firebase.google.com/docs/web/setup#available-libraries

    // Initialize Firebase
    const app = initializeApp(firebaseConfig);
    const analytics = getAnalytics(app);
  </script>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RNQHE0TJJL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-RNQHE0TJJL');
  </script>
</body>

</html>