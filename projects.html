<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  <!-- Favicon code from realfavicongenerator.net -->
  <link rel="apple-touch-icon" type="image/png" sizes="16x16" href="/assets/img/icons/rocket_icon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/rocket_icon.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#8b51a3">
  <meta name="msapplication-TileColor" content="#563d7c">
  <meta name="theme-color" content="#ffffff">

  <!--jQuery-->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"
    integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous">

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
<!--
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"
    integrity="sha384-oBqDVmMz9ATKxIep9tiCxS/Z9fNfEXiDAYTujMAeBAsjFuCZSmKbSSUnQlmh/jp3"
    crossorigin="anonymous"></script>


  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.min.js"
    integrity="sha384-7VPbUDkoPSGFnVtYi0QogXtr74QeVeeIs99Qfg5YCF+TidwNdjvaKZX19NZ/e6oz"
    crossorigin="anonymous"></script>
-->


  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css"
    integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css"
    integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
  <title>Hands-on Experience | Younggun Kim</title>
  <meta name="generator" content="Jekyll v4.2.2" />
  <meta property="og:title" content="Hands-on Experience" />
  <meta name="author" content="Younggun Kim" />
  <meta property="og:locale" content="en_GB" />
  <meta name="description" content="List of held Hands-on Experience" />
  <meta property="og:description" content="List of held Hands-on Experience" />
  <link rel="canonical" href="http://localhost:4000/projects.html" />
  <meta property="og:url" content="http://localhost:4000/projects.html" />
  <meta property="og:site_name" content="Younggun Kim" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta property="twitter:title" content="Hands-on Experience" />
  <meta name="twitter:site" content="@liu_puze" />
  <meta name="twitter:creator" content="@Younggun Kim" />
  <script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Younggun Kim"},"description":"List of held Hands-on Experience","headline":"Hands-on Experience","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/head.png"},"name":"Younggun Kim"},"url":"http://localhost:4000/projects.html"}</script>
  <!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="/assets/css/item.css">

</head>



<body>
  <header>

    <nav class="navbar navbar-expand-md bg-light fixed-top" style="background-color:#ffffff;">
      <div class="container-fluid mt-3">
        <button class="navbar-toggler my-auto" type="button" data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
                aria-label="Toggle navigation">
          <span><i class="fas fa-bars"></i></span>
        </button>
        <div class="collapse navbar-collapse mb-0 pb-3" id="navbarSupportedContent">
          <ul class="navbar-nav">
            <li class="nav-item btn-nav my-auto mx-auto pe-3 text-center">
              <a class="nav-link" href="/">
                <span data-hover="Home">Home</span>
              </a>
            </li>
            <li class="nav-item btn-nav my-auto mx-auto pe-3 text-center">
              <a class="nav-link" href="/publications.html">
                <span data-hover="Publications">Publications</span>
              </a>
            </li>
            <li class="nav-item dropdown"> 
                  <a class="nav-link dropdown-toggle" href="#" id="handsOnDropdown" role="button">
                    <span data-hover="Hands-on Experience">Hands-on Experience</span>
                  </a>
                
                  <ul class="dropdown-menu" aria-labelledby="handsOnDropdown">
                    <li><a class="dropdown-item" href="#CV">Computer Vision</a></li>
                    <li><a class="dropdown-item" href="#DL">Deep Learning</a></li>
                    <li><a class="dropdown-item" href="#MLLM">MLLM</a></li>
                    <li><a class="dropdown-item" href="#Robotics">Robotics</a></li>
                  </ul>
            </li>
          </ul>
        </div>
      </div>
    </nav>


  </header>



  <section class="page-title-bar" style="background-color: #2f6d7e; padding: 40px 0; ">
    <div class="container text-center">
      <h1 style="color: white; font-weight: 600; font-size: 2.2rem; margin: 0;">Hands-on Experience</h1>
    </div>
  </section>








<section id="CV" class="recentpubs" style="background-color: #F4F3EA;">
  <div class="recentpubs-wrap">
  </div>
</section>

<section id="recentpubs" class="recentpubs" style="background-color: #F4F3EA;">
  <div class="recentpubs-wrap">
    <p style="font-weight: bold; color: #000; font-size: 1.6rem;"> ðŸš¦ Computer Vision for ITS</p>
    <h3 class="bibliography">[1] Pedestrian Crossing Direction Prediction at Intersections for Pedestrian Safety</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim*</em></strong></span>, Mohamed Abdel-Aty, Keechoo Choi, Zubayer Islam, Dongdong Wang, and Shaoyan Zhai</p>
    <p style="font-size: 1.1rem; font-weight: 500;">ðŸŽ‰ <em>IEEE Open Journal of Intelligent Transportation Systems (OJ-ITS), 2025. [Impact Factor: 5.3, JCR Quartiles: Q1]</em></p>

    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/OJ_ITS_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> With the rise of smart intersections, accurately predicting a pedestrianâ€™s crossing direction at the intersection level is essential for enhancing pedestrian safety and optimizing traffic signal control. However, this task is challenging due to the diverse configurations of real-world intersections, each with different crosswalk orientations, geographic layouts, and CCTV placements and angles.</p>

      </div>

       <!-- Row 2: Methodology (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> To address this, we propose a geometric-invariant space embedding method. As illustrated in the video, our approach enables pedestrians captured at different intersections, with varying CCTV angles and crosswalk orientations, to be embedded into a unified spatial representation. This standardization allows robust learning of crossing behaviors. Furthermore, we adopt a Transformer-based encoder, achieving accuracy of 94.10% and an F1-score of 92.35%.</p>
        </div>
      </div>
    
      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> The proposed method demonstrates strong potential for real-world deployment. Its robustness to intersection variability makes it suitable for integration into city-wide intelligent traffic management systems to proactively ensure pedestrian safety at signalized intersections.</p>
        
          <div class="links mt-2">
            [<a href="https://ieeexplore.ieee.org/document/11016106" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
        </div>
      </div>
    </div>



    <h3 class="bibliography">[2] CCTV-Drone Calibration for Surrogate Safety Measure of Vehicles at Smart Intersections</h3>
    <p><strong><em>On going project</em></strong>
    <div class="row align-items-start">
      
      <!-- Drone_CCTV_Video.mp4 -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/Drone_CCTV_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> In smart intersections, accurately assessing vehicle-to-vehicle safety is crucial. This requires estimating <em>safety surrogate measures</em> such as relative distance. However, limited CCTV viewpoints, typically front-facing, lack depth information, making it infeasible to compute inter-vehicle distances directly from monocular footage.</p>
      </div>

       <!-- Row 2: Methodology (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> We calibrate top-down drone views with CCTV footage to construct vehicle bounding boxes in both front and bottom perspectives. By calibrating vehicles captured in drone views with those in CCTV footage, we develop a cross-view dataset that includes bottom bounding boxes. A model is then trained to estimate the both bounding boxes from CCTV-only views, enabling accurate spatial localization without relying on drone footage. </p>
        </div>
      </div>
    
      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong>  This work enables real-time vehicle safety monitoring using only existing CCTV infrastructure. By bridging the gap between aerial and CCTV footage, it allows accurate estimation of surrogate safety measures at intersections, contributing to proactive vehicle conflict detection and urban traffic safety management.</p>
        
        
        </div>
      </div>
    </div>




    <h3 class="bibliography">[3] Joint Trajectory Prediction of Vehicles and Pedestrians for Safety at Intersections</h3>
    <p><strong><em> Project under the National Science Foundation(NSF) and Center for Smart Streetscapes (CS3)</em></strong>
    <div class="row align-items-start">
      
      <!-- Drone_CCTV_Video.mp4 -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/Trajectory_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> Vehicles and pedestrians continuously influence each other's movement, particularly through behaviors such as yielding or hesitation. Accurately predicting the future trajectories of both agents is essential to improve road users' safety at intersections. A joint understanding enables proactive conflict detection and enhances traffic safety. </p>

         <p><strong>Methodology:</strong> We leverage a Graph Convolutional Network (GCN)-based framework to jointly encode the historical trajectories of vehicles and pedestrians into a shared latent feature space. The model captures mutual interactions between the two agent types and learns to predict their future trajectories concurrently.</p>
      </div>

      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> Our approach facilitates accurate multi-agent trajectory forecasting at intersections using observed movement patterns. By modeling pedestrianâ€“vehicle interactions, it enhances the reliability of safety assessment and supports the development of smarter traffic control systems that account for both pedestrians and vehicles.</p>
        
        
        </div>
      </div>
    </div>



  </div>
</section>



<section id="DL" class="recentpubs">
  <div class="recentpubs-wrap">
  </div>
</section>

<section id="recentpubs" class="recentpubs" >
  <div class="recentpubs-wrap">
    <p style="font-weight: bold; color: #000; font-size: 1.6rem;"> ðŸš˜ Novel Deep Learning Architectures for 3D LiDAR Recognition </p>
    <h3 class="bibliography">[1] 3D Adaptive Structural Convolution Network for Domain-Invariant Point Cloud Recognition</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span> and Soomok Lee*</p>
    <p style="font-size: 1.1rem; font-weight: 500;">ðŸŽ‰ <em>Asian Conference on Computer Vision (ACCV), 2024. [BK21(Brain Korea) Distinguished Conference Paper List]</em></p>
    
    <div class="row align-items-start">

      <div class="col-md-6">
        <img src="/assets/img/ASCN.png" alt="ASCN Figure" style="width: 100%; height: auto;" />
      </div>

      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> LiDAR is one of the crucial sensors for autonomous vehicles (AVs), offering accurate 3D spatial information essential for object recognition and safe navigation. However, the quality of LiDAR point clouds varies significantly with the number of channels (e.g., 32CH, 64CH, 128CH), and high-channel sensors are often too expensive for wide deployment. This creates a challenge, as deep learning models trained on high-resolution data often underperform when applied to lower-resolution inputs. Thus, it is critical to develop recognition models that are robust to LiDAR channel variations. </p>

      </div>

      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> We developed 3D-ASCN that recognizes objects using 3D point cloud data from LiDAR sensors. To address the challenges caused by variations in LiDAR sensor configurations, our model is designed to maintain reliable performance even when the quality or resolution of LiDAR data changes. Specifically, we introduce a distance-based kernel and a direction-based kernel that learn the structural feature representations of objects from point clouds. These kernels enable the model to focus on the intrinsic geometry of objects rather than the density of the point cloud itself, allowing for robust classification and object detection across different LiDAR resolutions.</p>
        </div>
      </div>
    
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> The proposed 3D-ASCN can contribute to the real-world deployment of AVs by enabling reliable object recognition regardless of the LiDAR sensor used. By overcoming performance degradation caused by LiDAR channel shifts, our approach reduces hardware dependency and cost, allowing AV systems to be both scalable and economically feasible. </p>
        
          <div class="links mt-2">
            [<a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Kim_3D_Adaptive_Structural_Convolution_Network_for_Domain-Invariant_Point_Cloud_Recognition_ACCV_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
        </div>
      </div>
    </div>



    <h3 class="bibliography">[2] MSCN: Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span>, Mohamed Abdel-Aty, Beomsik Cho, Seonghoon Ryoo, and Soomok Lee*</p>
    <p><em>This work has been submitted to IEEE Transactions on Vehicular Technology. [Impact Factor: 7.1, JCR Quartiles: Q1] </em></p>
    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <img src="/assets/img/MSCN.png" alt="MSCN Figure" style="width: 100%; height: auto;" />
      </div>

      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
        <ul style="padding-left: 1.2em; margin-top: 0.4rem;">
          <li style="margin-bottom: 0.4rem;">
            We developed MSCN, an improved version of the previously proposed 3D-ASCN model, which robustly recognizes objects from point clouds under both LiDAR channel variations and domain shifts between simulation and real-world environments.
          </li>
          <li style="margin-bottom: 0.4rem;">
            While 3D-ASCN focuses on local structural features centered around each point, MSCN extends this by capturing both <span style="text-decoration: underline;">local and global structural features</span>, leading to improved recognition performance and robustness to domain shifts.
          </li>
          <li style="margin-bottom: 0.4rem;">
            To evaluate the modelâ€™s recognition ability under simulation-to-real-world (Sim-to-Real) domain shifts, we developed a synthetic point cloud dataset. This dataset enabled rigorous testing of sim-to-real generalization, validating MSCNâ€™s robustness in diverse deployment scenarios.
          </li>
          <li style="margin-bottom: 0.4rem;">
            <strong>Impact:</strong> MSCN enables cost-effective deployment of AV systems by maintaining strong recognition performance across both varying LiDAR sensor configurations (e.g., low-cost vs. high-resolution sensors) and domain shifts between synthetic and real-world data, addressing key challenges in both sensor variability and data availability.
          </li>
        </ul>

      </div>

      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
          <div class="links mt-2">
            [<a href="https://arxiv.org/abs/2501.16289" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
      </div>
    </div>



  </div>
</section>





<section id="MLLM" class="recentpubs" style="background-color: #F4F3EA;">
  <div class="recentpubs-wrap">
  </div>
</section>
<section id="recentpubs" class="recentpubs" style="background-color: #F4F3EA;">
  <div class="recentpubs-wrap">
    <p style="font-weight: bold; color: #000; font-size: 1.6rem;"> ðŸ§  Multimodal Large Language Models for Visual Understanding and Safety</p>
    <h3 class="bibliography">[1] VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span>, Ahmed Abdelrahman*, and Mohamed Abdel-Aty</p>
    <p style="font-size: 1.1rem; font-weight: 500;">ðŸŽ‰ <em>International Conference on Computer Vision Workshop (ICCVW), 2025.</em></p>
    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/VRU_Accident_Video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> Vulnerable Road Users (VRUs), such as pedestrians and cyclists, are disproportionately affected in traffic accidents. Understanding the causes, contexts, and preventability of such accidents is crucial for improving road safety. Multimodal Large Language Models (MLLMs) have emerged as powerful tools for scene understanding and can support applications like accident report summarization and autonomous vehicle (AV) decision-making. However, their real-world utility remains limited due to a lack of high-quality, safety-focused benchmarks that test fine-grained reasoning and description capabilities in accident scenarios. </p>

      </div>

       <!-- Row 2: Methodology (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Benchmark Statistics:</strong> The VRU-Accident benchmark consists of <strong>1K</strong> real-world dashcam accident videos involving VRUs. It features <strong>6K</strong> multiple-choice video question answering (VQA) questions covering six categories: weather & lighting, traffic environment, road configuration, accident type, accident cause, and prevention measure. Each question comes with four answer choices, one correct and three counterfactual distractors, resulting in <strong>24K</strong> candidate options, including <strong>3.4K</strong> unique answers to encourage diverse and nuanced reasoning. The benchmark also provides <strong>1K</strong> densely annotated scene-level captions that narrate the spatiotemporal dynamics of accident scenarios.</p>
        </div>
      </div>
    
      <!-- Row 3: Impact (full width) -->
      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> VRU-Accident serves as the first large-scale benchmark to systematically evaluate MLLMs' understanding of VRU-related accident scenes through both VQA and dense captioning tasks. By providing diverse and semantically rich annotations grounded in real-world scenarios, the benchmark enables quantitative evaluation of MLLMs' reasoning, grounding, and narrative capabilities in high-risk traffic contexts. It lays the foundation for developing safer and more interpretable AI systems for AVs and transportation safety research. </p>
 
          <div class="links mt-2">
            [<a href="https://vru-accident.github.io/" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>

        </div>
      </div>
    </div>


  
    <h3 class="bibliography">[2] Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety</h3>
    <span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span>,&nbsp;Swetha Sirnam,&nbsp;Fazil Kagdi,&nbsp;and&nbsp;Mubarak Shah
    <p><em>This work has been submitted to Conference on Neural Information Processing Systems (NeurIPS). </em></p>
    <div class="row align-items-start">
      
      <!-- Left: Video -->
      <div class="col-md-6">
        <img src="/assets/img/Safe_LLaVA.png" alt="Safe_LLaVA Figure" style="width: 100%; height: auto;" />
      </div>

      <!-- Right: Description -->
      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> MLLMs have shown impressive capabilities across various domains. However, they pose serious privacy risks due to their tendency to leak biometric information such as race, gender, and age from visual data. Despite existing regulations like GDPR and SPCD, MLLMs often violate privacy constraints, which is particularly problematic in high-stakes applications such as government projects. This issue stems primarily from the leakage of biometric attributes in the training datasets used to build these models. </p>

         <p><strong>Methodology:</strong> We developed a novel pipeline to clean biometric information from open datasets used to train MLLMs. This process ensures that explicit biometric attributes are removed while preserving semantic richness. We also introduce a benchmark to evaluate both explicit leakage (where models are directly asked about biometric details) and implicit leakage (where personal information is inferred from open-ended questions). As illustrated in the figure, LLaVA-v1.5 trained on existing datasets reveals both types of leakage, whereas Safe-LLaVA refuses to answer biometric questions while maintaining rich, non-biometric descriptions in open-ended prompts.</p>

      </div>

      <div class="row mb-3">
        <div class="col">
          <p><strong>Impact:</strong> Safe-LLaVA offers the first comprehensive solution for training and evaluating privacy-aware MLLMs. It enables researchers and practitioners to develop models that are both biometrically safe and informative, which is critical for real-world deployment in domains requiring strict privacy compliance. By preventing both explicit and implicit biometric inference, Safe-LLaVA represents a key step toward building responsible and ethically aligned multimodal AI systems.</p>
        </div>
      </div>

      <div class="row mb-3">
          <div class="links mt-2">
            [<a href="https://github.com/Kimyounggun99/Safe-LLaVA" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
      </div>
    </div>
  </div>
</section>


<section id="Robotics" class="recentpubs">
  <div class="recentpubs-wrap">
  </div>
</section>
<section id="recentpubs" class="recentpubs" >
  <div class="recentpubs-wrap">
    <p style="font-weight: bold; color: #000; font-size: 1.6rem;"> ðŸ¤– Robotics Experience </p>
    <h3 class="bibliography">[1] Design of a Robotic Gripper capable of Grasping and Manipulating Various Objects</h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span>, Yooseong Lee, and Uikyum Kim*</p>
    <p style="font-size: 1.1rem; font-weight: 500;">ðŸŽ‰ <em>Best Paper Award at the 17th Korean Robotics Society Annual Conference (KROS)</em></p>
    <div class="row align-items-start">

      <div class="col-md-6">
        <a href="https://press.ajou.ac.kr/news/articleView.html?idxno=2901" target="_blank">
          <img src="/assets/img/News.png" alt="News" style="width: 100%; height: auto;" />
        </a>
      </div>

      <div class="col-md-6">
        <a href="https://press.ajou.ac.kr/news/articleView.html?idxno=2901" target="_blank">
          <img src="/assets/img/KROS.png" alt="KROS" style="width: 100%; height: auto;" />
        </a>
      </div>

      

      <div class="row mb-3">
        <div class="col">
          <p><strong>Motivation:</strong> Robotic grippers are widely used in industrial and research settings for object grasping. Conventional soft grippers such as the Fin Ray Effect (FRE) grippers can achieve stable adaptive grasping without complex control, but they have limitations in manipulating the pose or orientation of grasped objects. </p>
          <p><strong>Methodology:</strong> To address the limitations of soft grippers, we designed a novel gripper mechanism by adding an additional degree of freedom (DOF) to a standard FRE-inspired structure, enabling controlled deformation for manipulation tasks. We performed mathematical modeling and parametric analysis to determine the optimal force application points and directions for effective object manipulation. Then, we conducted Finite Element Analysis (FEA) simulations to validate deformation behavior under different load conditions.</p>
          <p><strong>Impact:</strong> The proposed gripper design enables simultaneous stable grasping and in-hand manipulation, overcoming the main limitations of conventional adaptive grippers. The gripper also provide a foundation for further development of adaptive robotic end-effectors capable of handling complex manipulation tasks in real-world environments. </p>
        </div>

      </div>


      
    </div>
    

    <h3 class="bibliography">[2] Implementation of a Well-known Robotic Gripper and Development of a Real-time Status Tracking System</h3>
    <p><strong><em>Capston project</em></strong>
    <div class="row align-items-start">

      <div class="container">
        <video width="100%" controls autoplay muted loop playsinline>
          <source src="/assets/videos/Capstone.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      <div class="row mb-3">
        <div class="col">
          <p><strong>Overview:</strong> This project was carried out as part of my undergraduate capstone experience, with the goal of integrating the fundamental principles of mechanical engineering, including kinematics, dynamics, and motor design, into a comprehensive robotic system. I was particularly inspired by the Omega Gripper, published in IEEE/ASME Transactions on Mechatronics, which was specifically designed to grasp challenging objects such as thin cards. Recognizing this as an excellent example of how core engineering knowledge could be combined into a solution, I adopted the Omega Gripper as the foundation of my work. To deepen my understanding, I visited the POSTECH March Lab and consulted directly with the first author of the original paper, which helped resolve technical questions and further motivated me to pursue this project. </p>

          <p><strong>Gripper Kinematic Design:</strong> The gripper mechanism features an underactuated structure implemented using a parallel four-bar mechanism. I derived and analyzed both the closed-loop forward and inverse kinematic equations to model the motion mathematically and ensure precise control over the gripperâ€™s configuration during operation. </p>

          <p><strong>Kinematic Simulation:</strong> Using MATLAB, I simulated the kinematics of the mechanism to optimize key parameters such as link lengths and ranges of motion, ensuring that the gripper could reliably adapt to a variety of object shapes and sizes. </p>
          <p><strong>Dynamic Simulation and Prototyping:</strong> Dynamic simulations were conducted to estimate the loads applied during grasping and to select appropriate material properties for the gripper body. Based on these analyses, I designed the gripper structure and fabricated the components via 3D printing. I also selected motors considering factors such as required grasping force and system responsiveness. </p>
          <p><strong>Demonstration and Real-time Tracking:</strong> For demonstration, I integrated an encoder at the lower part of the gripper and an IMU sensor at the center. The gripper was operated based on the inverse kinematic equations to achieve adaptive grasping of objects. Real-time tracking and visualization of the gripperâ€™s state were performed using forward kinematics, allowing continuous monitoring of its configuration during operation. </p>
        </div>
      </div>



    <h3 class="bibliography">[3] Intelligent cradle for a device </h3>
    <p><span style="text-decoration: underline;"><strong><em>Younggun Kim</em></strong></span>, Minjoung Sim, Hojun Lee, Wonjun Choi, and Hanbin Choi</p>
    <p style="font-size: 1.1rem; font-weight: 500;">ðŸŽ‰ <em>Granted Patent (Patent No. 10-2506732, KR) </em></p>
    <div class="row align-items-start">

      <div class="col-md-6">
          <img src="/assets/img/Face_mirroring.png" alt="Face_mirroring" style="width: 100%; height: auto;" />
      </div>

      <div class="col-md-6 text-start">
        
         <p><strong>Motivation:</strong> With the increasing adoption of IoT technologies in healthcare and daily life, there is a growing need for systems that can continuously monitor and assist patients or individuals with limited mobility in real time. Maintaining critical devices such as cameras or displays in the correct ergonomic position relative to the user's face is essential to improve comfort, safety, and the effectiveness of monitoring. To address this need, we developed an intelligent cradle mechanism capable of automatically tracking and adjusting its orientation based on head movements. </p>
      </div>

      <div class="row mb-3">
        <div class="col">
          <p><strong>Methodology:</strong> We designed a body-shaped ergonomic cradle that actively follows the userâ€™s head movements to keep the device consistently positioned in front of the face. We adopted a real-time face detection system to capture head pose and movement directions, and implemented motor control algorithms to dynamically adjust the cradleâ€™s orientation based on the detected motion. For deployment, we calculated the maximum load and structural requirements caused by body movements, and selected appropriate materials and actuators to ensure reliable performance.</p>

          <p><strong>Impact:</strong> This device enables hands-free operation and continuous monitoring, enhancing the usability of devices for patients, the elderly, or users with mobility constraints. We also demonstrates the practical integration of real-time vision-based tracking with adaptive mechanical systems, contributing to the development of smart IoT assistive devices. </p>
        </div>
      </div>


      <div class="row mb-3">
          <div class="links mt-2">
            [<a href="https://www.kipris.or.kr/khome/detail/newWindow.do" class="btn btn-sm z-depth-0" role="button">Learn More</a>]
          </div>
      </div>
      
    </div>







  </div>
</section>









  

  <!-- Footer -->
  <footer>
    <div class="footer" style="font-size: 1.2rem;">
      Contact: younggun.kim@ucf.edu
    </div>
  </footer>


  <script type="text/javascript" src="/assets/js/jquery-3.3.1.min.js"></script>
  <script type="text/javascript" src="/assets/js/Headroom.js"></script>
  <script type="text/javascript" src="/assets/js/jQuery.headroom.js"></script>
  <script type="text/javascript" src="/assets/js/custom.js"></script>


  <script>
    var divs = document.querySelectorAll("div.project-content");
    $('div.project-content').each(function (index) {
      var filename = $(this).text();
      filename = filename.replace(/\s/g, '');
      filename = filename.replace("/_posts/", "");
      filename = filename.replace(".md", ".html");
      filename = filename.substring(0, 4) + "/" + filename.substring(5);
      filename = filename.substring(0, 7) + "/" + filename.substring(8);
      filename = filename.substring(0, 10) + "/" + filename.substring(11);

      console.log(filename);
      $.ajax({
        url: filename, dataType: "html", success: function (data) {
          const node = new DOMParser().parseFromString(data, "text/html");
          divs[index].innerHTML = node.getElementById("post").getElementsByClassName("post-content")[0].innerHTML;
        }
      })
    });

  </script>
  </div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz"
    crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        // customised options
        // â€¢ auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true },
          { left: '$', right: '$', display: false },
          { left: '\\(', right: '\\)', display: false },
          { left: '\\[', right: '\\]', display: true }
        ],
        // â€¢ rendering keys, e.g.:
        throwOnError: false
      });
    });
  </script>
  <script type="module">
    // Import the functions you need from the SDKs you need
    import { initializeApp } from "https://www.gstatic.com/firebasejs/9.10.0/firebase-app.js";
    import { getAnalytics } from "https://www.gstatic.com/firebasejs/9.10.0/firebase-analytics.js";
    // TODO: Add SDKs for Firebase products that you want to use
    // https://firebase.google.com/docs/web/setup#available-libraries

    // Initialize Firebase
    const app = initializeApp(firebaseConfig);
    const analytics = getAnalytics(app);
  </script>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8HXMKG9C2K"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-8HXMKG9C2K');
  </script>
</body>

</html>